---
title: "Regression Project (Part 1)"
author: "Ramya Dhatri Vunikili (rdv253)"
date: "April 01, 2017"
output: pdf_document
keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Answer 1a

The scatter plot has been color coded to represent boys (0) in red and girls (1) in blue colors.
As we can see in the plot, there appears to be no different pattern for boys and girls.
```{r 1}
berkeley_data <- read.csv("BGS.csv")
n <- nrow(berkeley_data)

################################## 1a)  Scatter Plot #######################################

##Color coding for boys (red) and girls (green)
berkeley_data$Color <- as.factor(berkeley_data$Sex)
library(ggplot2)
ggplot(berkeley_data,aes(x=HT2, y=HT9))+
  geom_point(aes(colour=Color))+
  ggtitle("Heights At Age 2 vs Heights At Age 9")

```

## Answer 1b
```{r 1b}
########################## 1b) Linear Regression Of HT9 On HT2  ###########################
lm.ht9v2=lm(HT9 ~ HT2, data=berkeley_data)

## Beta Hat Matrix Calculation
## Creating X and Y matrices for the regression lm(HT9 ~ HT2)
X <- as.matrix(cbind(1,berkeley_data$HT2))
Y <- as.matrix(berkeley_data$HT9)

## Calculating matrix of estimated coefficients:
beta.matrix <- round(solve(t(X)%*%X)%*%t(X)%*%Y, digits=5)

## Labeling and organizing results into a data frame
beta.hat <- as.data.frame(cbind(c("Intercept","HT2"),beta.matrix))
names(beta.hat) <- c("Coefficient","Value")

## Calculating vector of residuals
res <- as.matrix(berkeley_data$HT9-beta.matrix[1]-beta.matrix[2]*berkeley_data$HT2)
k <- ncol(X)

## Calculating Variance-Covariance Matrix
CV = 1/(n-k) * as.numeric(t(res)%*%res) * solve(t(X)%*%X)

## Standard errors of the estimated coefficient HT2
StdErr = sqrt(diag(CV))

## Calculating p-value for a t-test of coefficient significance
P.Value = rbind(2*pt(abs(beta.matrix[1]/StdErr[1]), df=n-k,lower.tail= FALSE),
                2*pt(abs(beta.matrix[2]/StdErr[2]), df=n-k,lower.tail= FALSE))


## Concatenating into a single data.frame
beta.hat = cbind(beta.hat,StdErr,P.Value)
beta.hat



## Ho: b1 = 0; Ha: b1 != 0;
b1.pval = P.Value[2]
b1.pval
## At alpha = 0.05, b1.pval < alpha and hence we reject the null hypothesis b1 = 0.


## T-Statistic for b1
b1.tstat <- (beta.matrix[2]/StdErr[2])
b1.tstat

## F-statistic: 145.2 on 1 and 134 DF (from summary of the regression)
summary(lm.ht9v2)
fstat <- 145.2
sqrt(fstat) ## is 12.0499
## Calculated value of b1.tstat = 12.051 which is approx. equal to sqrt(fstat)
ggplot(berkeley_data,aes(x=HT2, y=HT9))+
  geom_point(aes(colour=Color))+
  geom_smooth(method=lm, se=FALSE)+
  ggtitle("Heights At Age 2 vs Heights At Age 9")
```

At alpha = 0.05, b1.pval (= 4.085436e-23) < alpha and hence we reject the null hypothesis that $\beta_1 = 0$

Calculated value of b1.tstat = 12.051 which is approximately equal to sqrt(fstat) i.e., 12.0499


## Normality & Homoscedasticity
``` {r 1b iii, echo=FALSE}
par(mfrow=c(2,2))
qqnorm(residuals(lm.ht9v2))
qqline(residuals(lm.ht9v2))
hist(residuals(lm.ht9v2))
plot(berkeley_data$HT2, residuals(lm.ht9v2))
abline(0,0)
```
Looking at the qqplot and histogram of residuals it can be concluded that the normality assumption has been satisfied for the linear regression.
Also, as the residual plot shows no funnel shaped pattern it can be said that the regression satisfies the homoscedaticity assumption too.

## Answer 1c (Seperate Slopes)
``` {r 1c, results='asis', echo=FALSE, warning=FALSE}
K1 = lm(HT9 ~ HT2 + Sex, data = berkeley_data)
library(pander)
panderOptions("digits", 6)
panderOptions('knitr.auto.asis', FALSE)
pander(K1)
a.k1 <- anova(K1, lm.ht9v2)
pander(a.k1)
```

As th p value for ANOVA between original model and the model with seperate slopes is 0.397051 ($\nless$ 0.05), we do not find any significant difference between the two models.

## Answer 1d (Seperate Slopes & Intercepts)
``` {r 1d, results='asis', echo=FALSE, warning=FALSE}
K2 = lm(HT9 ~ HT2 + Sex + Sex*HT2, data = berkeley_data)
library(pander)
panderOptions("digits", 6)
panderOptions('knitr.auto.asis', FALSE)
pander(K2)
a.k2 <- anova(K2, lm.ht9v2)
pander(a.k2)
```


As th p value for ANOVA between original model and the model with seperate slopes and intercepts is 0.60972 ($\nless$ 0.05), we do not find any significant difference between the two models.

## Answer 2a
``` {r 2a, echo=FALSE}
############################# 2a) Sctter Plot - HT18 On HT9 ###############################
##Scatterplot of heights at age 9 vs heights at age 2
##plot(berkeley_data$HT2, berkeley_data$HT9, main="Heights At Age 9 vs Heights At Age 2", 
## xlab = "Heights At Age 2", ylab = "Heights At Age 9", col = berkeley_data$Color)
ggplot(berkeley_data,aes(x=HT9, y=HT18))+
  geom_point(aes(colour=Color))+
  ggtitle("Heights At Age 18 vs Heights At Age 9")
```
Yes,there appears to be a different pattern of heights for boys and girls. At the age 18, boys seem to be taller than the girls at the same age. -------------------------------

## Answer 2b
``` {r 2b}
############################ 2b) Linear Regression Of HT18 On HT9 #########################
lm.ht18v9 = lm(HT18 ~ HT9, data = berkeley_data)


## Beta Hat Matrix Calculation
## Creating X and Y matrices for the regression lm(HT18 ~ HT9)
X <- as.matrix(cbind(1,berkeley_data$HT9))
Y <- as.matrix(berkeley_data$HT18)

## Calculating matrix of estimated coefficients:
beta.matrix <- round(solve(t(X)%*%X)%*%t(X)%*%Y, digits=5)

## Labeling and organizing results into a data frame
beta.hat <- as.data.frame(cbind(c("Intercept","HT9"),beta.matrix))
names(beta.hat) <- c("Coefficient","Value")

## Calculating vector of residuals
res <- as.matrix(berkeley_data$HT18-beta.matrix[1]-beta.matrix[2]*berkeley_data$HT9)
k <- ncol(X)

## Calculating Variance-Covariance Matrix
CV = 1/(n-k) * as.numeric(t(res)%*%res) * solve(t(X)%*%X)

## Standard errors of the estimated coefficient HT9
StdErr = sqrt(diag(CV))

## Calculating p-value for a t-test of coefficient significance
P.Value = rbind(2*pt(abs(beta.matrix[1]/StdErr[1]), df=n-k,lower.tail= FALSE),
                2*pt(abs(beta.matrix[2]/StdErr[2]), df=n-k,lower.tail= FALSE))


## Concatenating into a single data.frame
beta.hat = cbind(beta.hat,StdErr,P.Value)
beta.hat
ggplot(berkeley_data,aes(x=HT9, y=HT18))+
  geom_point(aes(colour=Color))+
  geom_smooth(method=lm, se=FALSE)+
  ggtitle("Heights At Age 18 vs Heights At Age 9")
```



## Answer 2c (Seperate Slopes)
``` {r 2c, results='asis', echo=FALSE, warning=FALSE}
T1 = lm(HT18 ~ HT9 + Sex, data = berkeley_data)
library(pander)
panderOptions("digits", 6)
panderOptions('knitr.auto.asis', FALSE)
pander(T1)
a1 <- anova(T1, lm.ht18v9)
pander(a1)
```

As the p value < 0.05, we can conclcude that the two models lm.ht18v9 and T1 differ significantly and hence T1 is a better fit to the data.


## Answer 2d (Seperate Intercepts and Slopes)
``` {r 2d, results='asis', echo=FALSE}
T2 = lm(HT18 ~ HT9 + Sex + HT2:Sex, data = berkeley_data)
library(pander)
panderOptions("digits", 6)
panderOptions('knitr.auto.asis', FALSE)
pander(T2)
a2 <- anova(T2, lm.ht18v9)
pander(a2)
```

As the p value < 0.05, we can conclcude that the two models lm.ht18v9 and T2 differ significantly and hence T2 is a better fit to the data.



## Answer 2e
``` {r 2e, results='asis'}
a3 <- anova(T2, T1)
library(pander)
panderOptions("digits", 6)
panderOptions('knitr.auto.asis', FALSE)
pander(a3)
########################### Parameter Estimates For The Best Model ########################
pander(T1)
```

As th p value for anova(T2,T1) is 0.884 ($\nless$ 0.05), we do not find any significant difference between the two models with different slopes and different slopes and intercepts. Moreover, the RSS values of the two models are nearly equal.



## Answer 3a
``` {r 3a}
################################# 3a) M1: WT18 ON WT9 ######################################
M1 <- lm(WT18 ~ WT9, data = berkeley_data)
summary(M1)
ggplot(berkeley_data,aes(x=WT9, y=WT18))+
  geom_point(aes(colour=Color))+
  geom_smooth(method=lm, se=FALSE)+
  ggtitle("Weights At Age 18 vs Weights At Age 9")
############################ 3a) M1: WT18 ON WT9 AND LG9 ###################################
M2 <- lm(WT18 ~ WT9 + LG9, data = berkeley_data)
summary(M2)
```

## Correlation
``` {r 3aii, echo=FALSE}
cor(berkeley_data$WT9,berkeley_data$LG9)
ggplot(berkeley_data,aes(x=WT9, y=LG9))+
  geom_point(aes(colour=Color))+
  geom_smooth(method=lm, se=FALSE)+
  ggtitle("Weights At Age 9 vs Leg Circumference At Age 9 (Highly Correlated)")
```
The correlation coefficient between LG9 and WT9 is found to be 0.9204. This implies that WT9 and LG9 exhibit a strong positive linear relationship. So, we can drop one of the predictors while regressing on HT18. This is because a model with two highly correlated predictors imparts nearly the same information to the regression model. But, by including both we are actually weakening the model. We are not adding incremental information. Instead, we are infusing your model with noise. Hence, considering WT9 would be enough in the regression model.


## Answer 3b
This matrix is called as the hat matrix because it puts a hat on the column vector $\textbf{Y} = (Y_1, Y_2,....,Y_n)^T$. This means that it transforms vector of observed responses ($\mathbf{Y}$) into a vector of fitted responses ($\mathbf{\hat{Y}}$) using
$\mathbf{\hat{Y}} = \mathbf{HY}$.




## Answer 3b ii
For Regression M1, 
``` {r 3b ii}
############################ 3b ii) Hat Matrix For M1  #####################################
X.M1 <- as.matrix(cbind(1,berkeley_data$WT9))
H.M1 <- round(X.M1 %*% solve(t(X.M1) %*% X.M1) %*% t(X.M1), digits=5)
sum(diag(H.M1))
max(diag(H.M1))
which.max(diag(H.M1))

############################ 3b ii) Hat Matrix For M2  #####################################
X.M2 <- as.matrix(cbind(1,berkeley_data$WT9,berkeley_data$LG9))
H.M2 <- round(X.M2 %*% solve(t(X.M2) %*% X.M2) %*% t(X.M2), digits=5)
sum(diag(H.M2))
max(diag(H.M2))
```
The Hat matrix (H1) has be calculated for boys data and one of the elements of H1 had a high leverage of 0.26 while the other elements of the order $10_{-3}$. 

## Answer 3b iii
``` {r 3b iii, results='asis', warning=FALSE, cache = FALSE, message=FALSE}
######################## 3b iii) Boys Data With High Leverage Point ########################
boys_data <- subset(berkeley_data, berkeley_data$Sex=="0")
B1 <- lm(WT18 ~ WT9, data = boys_data)
ggplot(boys_data,aes(x=WT9, y=WT18))+
  geom_point(aes(colour=Color))+
  geom_smooth(method=lm, se=FALSE)+
  ggtitle("Boys - Weights At Age 18 vs Weights At Age 9")
summary(B1)
library(pander)
panderOptions("digits", 6)
panderOptions('knitr.auto.asis', FALSE)
pander(B1)




###################### 3b iii) Boys Data Without High Leverage Point #######################
boys_without_high_lvg <- boys_data[-(which.max(diag(H.M1))),]
B2 <- lm(WT18 ~ WT9, data = boys_without_high_lvg)
ggplot(boys_without_high_lvg,aes(x=WT9, y=WT18))+
  geom_point(aes(colour=Color))+
  geom_smooth(method=lm, se=FALSE)+
  ggtitle("Boys - Weights At Age 18 vs Weights At Age 9 (Without High Leverage Point)")
summary(B2)
pander(B2)
```

The regression coefficient of WT9 in B1 is 1.04808 with a t value of 6.79611. Whereas, the same in B2 is 1.66669 with a t value of 8.63939. Hence, it is found to be statistically signifcant in B2.

## Answer 3b iv
``` {r 3b iv}
################### 3b iv) Regression With & Without High Leverage Point ###################
ggplot(boys_data,aes(x=WT9, y=WT18))+
  geom_point()+
  geom_smooth(data=boys_data, aes(x=WT9, y=WT18, colour="green"), method=lm, se=FALSE)+
  geom_smooth(data=boys_without_high_lvg, aes(x=WT9, y=WT18, colour="blue"), method=lm, se=FALSE)+
  scale_colour_manual(values = c("green", "blue"), labels = c("Without High Leverage", "With High Leverage"))+
  ggtitle("Boys - Weights At Age 18 vs Weights At Age 9")
```


## Answer 3b v
The $R^2$ and adjusted $R^2$ values for the model without the high leverage point(B2) have been found to be higher than those of the model B1. Hence, B2 can be considered as a better fit to the data.
``` {r 3b v, results='asis', echo=FALSE}
library(pander)
panderOptions("digits", 6)
panderOptions('knitr.auto.asis', FALSE)
pander(B2)
``` 

## Answer 4a
``` {r 4a, echo = FALSE, results="hide", warning=FALSE, cache = FALSE, message=FALSE}
soma.plot1 <- ggplot(berkeley_data,aes(x=WT2, y=Soma))+
  geom_point(aes(colour=Color))+
  geom_smooth(method=lm, se=FALSE)+
  ggtitle("Soma vs WT2")

soma.plot2 <- ggplot(berkeley_data,aes(x=WT9, y=Soma))+
  geom_point(aes(colour=Color))+
  geom_smooth(method=lm, se=FALSE)+
  ggtitle("Soma vs WT9")

soma.plot3 <- ggplot(berkeley_data,aes(x=WT18, y=Soma))+
  geom_point(aes(colour=Color))+
  geom_smooth(method=lm, se=FALSE)+
  ggtitle("Soma vs WT18")

##install.packages("cowplot")
library("cowplot")
plot_grid(soma.plot1, soma.plot2, soma.plot3, align = "v", nrow = 2, ncol=2)
```
The response variable Somatotype exhibits a negative relationship against weight at the age of 2. However, it is seen to be increasing with weights at age 9 and 18 with the regression line being steeper against WT9. The relationship between somatotype and WT9 would have been stronger if the high leverage point in WT9 was not considered.


## Answer 4b
 
``` {r 4b}
berkeley_data$DW9 <- (berkeley_data$WT9 - berkeley_data$WT2)
berkeley_data$DW18 <- (berkeley_data$WT18 - berkeley_data$WT9)
berkeley_data$AVE <- (berkeley_data$WT2 + berkeley_data$WT9 + berkeley_data$WT18)/3
berkeley_data$LIN <- (berkeley_data$WT18 - berkeley_data$WT2)
berkeley_data$QUAD <- (berkeley_data$WT2 - 2*berkeley_data$WT9 + berkeley_data$WT18)
``` 

## Answer 4c i
 
``` {r 4c, results='asis', warning=FALSE, cache = FALSE, message=FALSE}
M1 <- lm(Soma ~ WT2 +WT9 +WT18, data = berkeley_data)
M2 <- lm(Soma ~ WT2 + DW9 + DW18, data = berkeley_data)
M3 <- lm(Soma ~ AVE + LIN + QUAD, data = berkeley_data)
summary(M1)
summary(M2)
summary(M3)
#install.packages("stargazer")
#library(stargazer)
#stargazer(M1, M2, M3, title="Results", align=TRUE, type = "latex")

#install.packages("pander")
#install.packages("memisc")
library(pander)
panderOptions("digits", 6)
panderOptions('knitr.auto.asis', FALSE)
pander(M1)
pander(M2)
pander(M3)
#library(memisc)
#panderOptions('table.alignment.rownames', 'left')
#pander(mtable(M1,M2,M3))

#install.packages("texreg")
#install.packages("xtable")
#library(xtable)
#library(texreg)
#print(xtable(M1), type = "html")
```
Same Regression Coefficients:
\newline \textbf{Intercept} in M1, M2 and M3
\newline \textbf{WT18} in M1 and \textbf{DW18} in M2
\newline \textbf{WT2} in M2 and \textbf{AVE} in M3

## Answer 4c ii

In Model 1, the estimate for WT18 is the effect on Somatotype of changing WT18 by one unit, with all other terms held fixed. In Model 2, the estimate for DW18 is the change in Somatotype when DW18 changes by one unit, when all other terms are held fixed. But the only way $DW18 = WT18 - WT9$ can be changed by one unit with the other variables including $WT9 = DW9 - WT2$ held fixed is by changing WT18 by one unit. Consequently, the terms WT18 in Model 1 and DW18 in Model 2 are same.

In Model 1, the estimate for WT9 is the effect on Somatotype of changing WT9 by one unit, with WT2 and WT18 fixed. In Model 2, the estimate for DW9 is the change in Somatotype when DW9 changes by one unit, when DW18 and WT2 are fixed. As $DW9 = WT9 - WT2$, the only way to change DW9 is by changing WT9 as WT2 is held constant. But in order to keep $DW18 = WT18 - WT9$ constant, we need to change WT18 with a corresponding change in WT2. Hence, the difference in estimates for WT9 in M1 and DW9 in M2 is accounted to the variation in WT18 in M2 while WT18 and WT2 are both held constant in M1.


## Answer 4c iii
Let the regression coefficients of model M2 be $\gamma_0, \gamma_1, \gamma_2 and \gamma_3$. Thus, the model M2 can be represented by the below equation:
\newline $$\begin{aligned} Somatotype &= \gamma_0 + \gamma_1AVE + \gamma_2LIN + \gamma_3QUAD\\ Somatotype &=\gamma_0 + \frac{\gamma_1}{3}(WT2 + WT9 + WT18) + \gamma_2(WT18 - WT2) + \gamma_3(WT2 - 2*WT9 +WT18)\\ Somatotype &= \gamma_0 + WT2(\frac{\gamma_1}{3} - \gamma_2 + \gamma_3) + WT9(\frac{\gamma_1}{3} - 2\gamma_3) + WT18(\frac{\gamma_1}{3} + \gamma_2 + \gamma_3)\\ \end{aligned}$$
\newline Let the regression coefficients of model M2 be $\beta_0, \beta_1, \beta_2 and \beta_3$. Then M1 ca be represented as
\newline $$\begin{aligned} Somatotype &= \beta_0 + \beta_1WT2 + \beta_2WT9 + \beta_3WT18\\ \end{aligned}$$
\newline Hence by comparing the two models we can say that
\newline $$\begin{aligned} \beta_0 &= \gamma_0\\ \beta_1 &= (\frac{\gamma_1}{3} - \gamma_2 + \gamma_3)\\ \beta_2 &= (\frac{\gamma_1}{3} - 2\gamma_3)\\ \beta_3 &= (\frac{\gamma_1}{3} + \gamma_2 + \gamma_3)\\ \end{aligned}$$


## Answer 4d
 
``` {r 4d, warning=FALSE, cache = FALSE, message=FALSE}
M4 <- lm(Soma ~ WT2 +WT9 +WT18 +DW9, data = berkeley_data)
summary(M4)
#panderOptions("digits", 6)
#panderOptions('knitr.auto.asis', FALSE)
#pander(M4)
```
As the number of linearly independent quantitites is three, we cannot use more than three linear combinations of the predictors. Since DW9 can be written as an exact linear combination of the other predictors, DW9 = WT9 - WT2, the residuals from this  regression are all exactly zero. A slope coefficient for DW9 is not defined after adjusting for the other three terms. Hence, the four terms WT2, WT9, WT18, and DW9 are linearly dependent, since one can be determined exactly from the others.
\newline Thus, the regression coefficient of DW9 is set to "NA" to indicate that the predictor has been aliased and hence not estimated.
